# üöÄ Cluster HPC TECHNE ‚Äî Documenta√ß√£o T√©cnica Completa

Este reposit√≥rio documenta a arquitetura, configura√ß√£o e infraestrutura do **Cluster HPC TECHNE**, utilizado para processamento de alto desempenho (HPC) com gerenciamento via **Slurm**.

---

## üìå 1. Vis√£o Geral e Arquitetura

O cluster TECHNE √© composto por um n√≥ controlador e m√∫ltiplos n√≥s de execu√ß√£o heterog√™neos, com suporte a GPUs NVIDIA L4, armazenamento compartilhado e monitoramento centralizado.

### üîß Componentes Principais

| Componente | Detalhes T√©cnicos |
|-----------|-------------------|
| **Controlador / Master** | `slurm-master` ‚Äî IP: `10.xx.yy.zz`<br>Servi√ßos: Slurmctld, Slurmdbd, PostgreSQL/MariaDB, Munge |
| **N√≥ de Execu√ß√£o 1** | `gpunode01` ‚Äî 16 Cores, 62.9 GB RAM<br>2x GPUs NVIDIA L4 |
| **N√≥ de Execu√ß√£o 2** | `gpunode02` ‚Äî 12 Cores, 31.0 GB RAM<br>1x GPU NVIDIA L4 |
| **Sistema Operacional** | Linux Ubuntu/Debian ‚Äî Kernel 6.8.x |
| **Armazenamento** | NFS em `/data/` + LVM no disco principal |

### üñ•Ô∏è Configura√ß√£o de Hardware (via `lshw`)

- **CPU:** Intel¬Æ Xeon¬Æ Gold 6526Y (2 sockets l√≥gicos)  
- **RAM Total:** 32 GiB (62.9 GiB dispon√≠veis ao Slurm via `RealMemory`)  
- **GPUs:** 2√ó NVIDIA L4 (AD104GL) ‚Äî 23 GB VRAM cada  
- **Controladoras:** Virtio SCSI e Virtio Network  

---

## üì° 2. Configura√ß√£o do Agendador Slurm

O Slurm √© configurado de modo centralizado e replicado em todos os n√≥s,
utilizando Munge para autentica√ß√£o.

### üìÑ 2.1. `slurm.conf` (Trecho Principal)

``` ini
# Configura√ß√£o Central
ControlMachine=slurm-master
ControlAddr=10.xx.xx.xx
SlurmUser=slurm
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge

# Contabilidade
JobCompType=jobcomp/none
JobAcctGatherType=jobacct_gather/cgroup
AccountingStorageHost=slurm-master

# Defini√ß√£o de N√≥s e Parti√ß√µes
NodeName=gpunode01 NodeAddr=10.9x.xx.xx Gres=gpu:2 State=IDLE
NodeName=gpunode02 NodeAddr=10.9x.xx.xx Gres=gpu:1 State=IDLE

PartitionName=gpu_part Nodes=gpunode01,gpunode02 Default=YES
```

### üìä 2.2. Contabilidade e Logs

-   **slurmdbd** executando no controlador.
-   Bancos:
    -   **MariaDB** ‚Üí Slurm Accounting\
    -   **PostgreSQL** ‚Üí M√©tricas do monitoramento / Grafana
-   Usu√°rio **manager** com `AdminLevel=Manager` no `sacctmgr`.

------------------------------------------------------------------------

## üìà 3. Pipeline de Monitoramento (Customizado)

O cluster possui um pipeline pr√≥prio de coleta e visualiza√ß√£o de
m√©tricas via Python + PostgreSQL + Grafana.

### üêç 3.1. Agente Python (`collect_metrics.py`)

-   Local: `/opt/cluster_monitor/collect_metrics.py`
-   Execu√ß√£o: a cada **1 minuto** (via `cron`)
-   Fun√ß√µes:
    -   coleta completa do estado do cluster (jobs, GPU, CPU, RAM)
    -   normaliza√ß√£o dos dados
    -   envio ao PostgreSQL

#### ‚úî Corre√ß√£o Importante

Para compatibilidade com CUDA:

``` bash
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/lib/x86_64-linux-gnu
```

Sem isso, PyTorch n√£o encontra as bibliotecas CUDA.

### üóÑÔ∏è 3.2. Estrutura do Banco (PostgreSQL)

  -----------------------------------------------------------------------
  Tabela                   Armazena                       Uso
  ------------------------ ------------------------------ ---------------
  **gpu_log**              Utiliza√ß√£o, mem√≥ria usada,     Gr√°ficos
                           temperatura                    temporais de
                                                          GPU

  **job_log**              Hist√≥rico de jobs (JobID,      Auditoria e
                           runtime, state)                estat√≠sticas

  **queue_state**          Contagem de jobs por estado    Status da fila
                                                          no Grafana

  **utilization**          Uso de CPU (%) e RAM (%) por   Pain√©is de
                           n√≥                             ocupa√ß√£o
  -----------------------------------------------------------------------

### üìä 3.3. Dashboards no Grafana

-   **Jobs por Estado** ‚Üí Gr√°fico de barras
-   **Uso da GPU** ‚Üí Time series (GPU 0 / GPU 1)
-   **Uso de Disco** ‚Üí Gauge (percentual)

------------------------------------------------------------------------

## üß© 4. Tecnologias Utilizadas (Active Stack)

  Categoria             Tecnologias
  --------------------- -------------------------------------------------
  **Gerenciamento**     Slurm 23.11.4, Munge, systemd
  **Acelera√ß√£o**        NVIDIA Drivers 570.x, CUDA 12.0/12.8, cuDNN 8.9
  **Desenvolvimento**   Python 3.12, venv
  **Bibliotecas**       psycopg2, psutil, subprocess, re, python-dotenv
  **Bancos de Dados**   PostgreSQL, MariaDB
  **Rede**              SSH/SCP, ufw

------------------------------------------------------------------------

## üìö Licen√ßa

Este documento faz parte da infraestrutura interna do Cluster TECHNE e
pode ser reutilizado para estudos, configura√ß√£o e expans√£o do ambiente.

------------------------------------------------------------------------

## ‚ú® Contato

**INFRA NCAD / UFPI**\
Gerenciamento e Desenvolvimento do Cluster HPC TECHNE

